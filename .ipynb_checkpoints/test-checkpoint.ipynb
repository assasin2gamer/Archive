{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978528e-8092-47c3-96ff-deae1c7aff3b",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations, islice\n",
    "from sklearn.decomposition import FastICA\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from numba import njit\n",
    "import cupy as cp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Workaround for torch_geometric compatibility ---\n",
    "if not hasattr(torch.serialization, 'add_safe_globals'):\n",
    "    torch.serialization.add_safe_globals = lambda x: None\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ===================== Propagation Graph Implementation =====================\n",
    "# These functions come directly from the provided code.\n",
    "@njit\n",
    "def cost(a, b):\n",
    "    return abs(a - b)\n",
    "\n",
    "@njit\n",
    "def compute_d_matrix(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = np.empty((T, T), dtype=np.float64)\n",
    "    D[0, 0] = cost(s[0], t[0])\n",
    "    for j in range(1, T):\n",
    "        D[0, j] = D[0, j-1] + cost(s[0], t[j])\n",
    "    for i in range(1, T):\n",
    "        D[i, 0] = D[i-1, 0] + cost(s[i], t[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            a_val = D[i-1, j]\n",
    "            b_val = D[i, j-1]\n",
    "            c_val = D[i-1, j-1]\n",
    "            D[i, j] = min(a_val, b_val, c_val) + cost(s[i], t[j])\n",
    "    return D\n",
    "\n",
    "@njit\n",
    "def compute_F_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    F = np.zeros((T, T), dtype=np.float64)\n",
    "    F[0, 0] = 1.0\n",
    "    for i in range(T):\n",
    "        for j in range(T):\n",
    "            if i == 0 and j == 0:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            c = cost(s[i], t[j])\n",
    "            if i > 0 and abs(D[i, j] - (D[i-1, j] + c)) < 1e-8:\n",
    "                total += F[i-1, j]\n",
    "            if j > 0 and abs(D[i, j] - (D[i, j-1] + c)) < 1e-8:\n",
    "                total += F[i, j-1]\n",
    "            if i > 0 and j > 0 and abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total += F[i-1, j-1]\n",
    "            F[i, j] = total\n",
    "    return F\n",
    "\n",
    "@njit\n",
    "def compute_B_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    B = np.zeros((T, T), dtype=np.float64)\n",
    "    B[T-1, T-1] = 1.0\n",
    "    for i in range(T-1, -1, -1):\n",
    "        for j in range(T-1, -1, -1):\n",
    "            if i == T-1 and j == T-1:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            if i+1 < T and abs(D[i+1, j] - (D[i, j] + cost(s[i+1], t[j]))) < 1e-8:\n",
    "                total += B[i+1, j]\n",
    "            if j+1 < T and abs(D[i, j+1] - (D[i, j] + cost(s[i], t[j+1]))) < 1e-8:\n",
    "                total += B[i, j+1]\n",
    "            if i+1 < T and j+1 < T and abs(D[i+1, j+1] - (D[i, j] + cost(s[i+1], t[j+1]))) < 1e-8:\n",
    "                total += B[i+1, j+1]\n",
    "            B[i, j] = total\n",
    "    return B\n",
    "\n",
    "@njit\n",
    "def avg_time_delay(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = compute_d_matrix(s, t)\n",
    "    F = compute_F_matrix(s, t, D)\n",
    "    B = compute_B_matrix(s, t, D)\n",
    "    total_delay = 0.0\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            c = cost(s[i], t[j])\n",
    "            if abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total_delay += (j - i) * F[i-1, j-1] * B[i, j]\n",
    "    num_alignments = B[0, 0]\n",
    "    if num_alignments == 0:\n",
    "        return 0.0\n",
    "    return total_delay / num_alignments\n",
    "\n",
    "def avg_time_delay_gpu(s, t):\n",
    "    s_gpu = cp.asarray(s)\n",
    "    t_gpu = cp.asarray(t)\n",
    "    T = s_gpu.shape[0]\n",
    "    D_gpu = cp.zeros((T, T), dtype=cp.float64)\n",
    "    D_gpu[0, 0] = cp.abs(s_gpu[0] - t_gpu[0])\n",
    "    for j in range(1, T):\n",
    "        D_gpu[0, j] = D_gpu[0, j-1] + cp.abs(s_gpu[0] - t_gpu[j])\n",
    "    for i in range(1, T):\n",
    "        D_gpu[i, 0] = D_gpu[i-1, 0] + cp.abs(s_gpu[i] - t_gpu[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            D_gpu[i, j] = cp.minimum(cp.minimum(D_gpu[i-1, j], D_gpu[i, j-1]),\n",
    "                                       D_gpu[i-1, j-1]) + cp.abs(s_gpu[i] - t_gpu[j])\n",
    "    D = cp.asnumpy(D_gpu)\n",
    "    return avg_time_delay(s, t)\n",
    "\n",
    "def build_propagation_graph(signal_list):\n",
    "    \"\"\"\n",
    "    Construct a propagation graph from a list of stock signals.\n",
    "    Each signal is a 1D numpy array.\n",
    "    Returns:\n",
    "        graph: dict mapping stock index to list of neighbor indices.\n",
    "        delays: dict mapping (i, j) to average delay.\n",
    "    \"\"\"\n",
    "    N = len(signal_list)\n",
    "    edges = []\n",
    "    delays = {}\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            delay_val = avg_time_delay(np.array(signal_list[i]), np.array(signal_list[j]))\n",
    "            if np.isclose(delay_val, 0.0):\n",
    "                continue\n",
    "            if delay_val > 0:\n",
    "                edges.append((i, j))\n",
    "                delays[(i, j)] = delay_val\n",
    "            else:\n",
    "                edges.append((j, i))\n",
    "                delays[(j, i)] = -delay_val\n",
    "    graph = {}\n",
    "    for (u, v) in edges:\n",
    "        graph.setdefault(u, []).append(v)\n",
    "    return graph, delays\n",
    "\n",
    "def draw_graph(graph, delays, title=\"Propagation Graph\", filename=\"graph.png\"):\n",
    "    G = nx.DiGraph()\n",
    "    for u, vs in graph.items():\n",
    "        for v in vs:\n",
    "            G.add_edge(u, v, weight=delays.get((u, v), 0))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", arrowstyle=\"->\", arrowsize=20)\n",
    "    edge_labels = {(u, v): f\"{data['weight']:.2f}\" for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def convert_prop_graph_to_edge_tensors(delays):\n",
    "    \"\"\"\n",
    "    Convert the delays dictionary into edge_index and edge_weight tensors.\n",
    "    We use exp(-delay) as weight.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    weights = []\n",
    "    for (u, v), delay in delays.items():\n",
    "        edges.append([u, v])\n",
    "        weights.append(np.exp(-delay))\n",
    "    edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "    edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "# ===================== Feature Engineering & Dataset =====================\n",
    "def compute_features(prices, window=5):\n",
    "    \"\"\"\n",
    "    Compute log return, rolling mean, and rolling std.\n",
    "    Returns a DataFrame with MultiIndex columns (stock, feature).\n",
    "    \"\"\"\n",
    "    log_ret = np.log(prices) - np.log(prices.shift(1))\n",
    "    rolling_mean = log_ret.rolling(window=window).mean()\n",
    "    rolling_std = log_ret.rolling(window=window).std()\n",
    "    \n",
    "    arrays = []\n",
    "    for stock in prices.columns:\n",
    "        for feat in ['log', 'mean', 'std']:\n",
    "            arrays.append((stock, feat))\n",
    "    col_index = pd.MultiIndex.from_tuples(arrays, names=['stock', 'feature'])\n",
    "    feature_df = pd.DataFrame(index=prices.index, columns=col_index)\n",
    "    for stock in prices.columns:\n",
    "        feature_df[(stock, 'log')] = log_ret[stock]\n",
    "        feature_df[(stock, 'mean')] = rolling_mean[stock]\n",
    "        feature_df[(stock, 'std')] = rolling_std[stock]\n",
    "    feature_df = feature_df.dropna()\n",
    "    return feature_df\n",
    "\n",
    "class StockFeatureDataset(Dataset):\n",
    "    def __init__(self, feature_df, window_size):\n",
    "        \"\"\"\n",
    "        feature_df: DataFrame with MultiIndex columns (stock, feature)\n",
    "        window_size: number of historical days.\n",
    "        For each sample, for each stock, extract a window of features (shape: (window_size, num_features)),\n",
    "        flatten it, and use as input.\n",
    "        Target: next-day log return for each stock.\n",
    "        \"\"\"\n",
    "        self.feature_df = feature_df\n",
    "        self.window_size = window_size\n",
    "        self.stocks = feature_df.columns.levels[0].tolist()\n",
    "        self.features = feature_df.columns.levels[1].tolist()\n",
    "        self.num_features = len(self.features)\n",
    "        self.num_stocks = len(self.stocks)\n",
    "        self.num_samples = feature_df.shape[0] - window_size\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, idx):\n",
    "        X_list = []\n",
    "        targets = []\n",
    "        for stock in self.stocks:\n",
    "            stock_data = self.feature_df[stock].iloc[idx: idx + self.window_size].values\n",
    "            X_list.append(stock_data.flatten())\n",
    "            targets.append(self.feature_df[stock].iloc[idx + self.window_size]['log'])\n",
    "        X = np.vstack(X_list)\n",
    "        y = np.array(targets)\n",
    "        return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# ===================== KAN Model Definition =====================\n",
    "class StockKAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "        super(StockKAN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gat = GATConv(hidden_dim, hidden_dim, heads=heads, concat=False)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.encoder(x)\n",
    "        x = self.gat(x, edge_index)\n",
    "        out = self.predictor(x)\n",
    "        return out\n",
    "\n",
    "# ===================== Main Pipeline =====================\n",
    "def main():\n",
    "    # Load price data\n",
    "    df = pd.read_csv('./data/global_titans.csv')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    stocks = df.columns.tolist()\n",
    "    \n",
    "    # Compute features: log return, rolling mean, and rolling std (window=5)\n",
    "    features_df = compute_features(df, window=5)\n",
    "    \n",
    "    # Split into training and testing sets (first half for training, second half for testing)\n",
    "    T = features_df.shape[0]\n",
    "    train_features = features_df.iloc[:T//2]\n",
    "    test_features = features_df.iloc[T//2:]\n",
    "    \n",
    "    # For propagation graph, use training log returns (from features)\n",
    "    train_returns = train_features.xs('log', level='feature', axis=1)\n",
    "    train_data = train_returns.values\n",
    "    n_components = min(3, train_data.shape[1])\n",
    "    ica = FastICA(n_components=n_components, random_state=0, max_iter=10000, tol=1e-3)\n",
    "    try:\n",
    "        ica.fit(train_data)\n",
    "    except Exception as e:\n",
    "        print(\"FastICA failed on training data:\", e)\n",
    "        return\n",
    "    \n",
    "    # Build propagation graph using the provided implementation:\n",
    "    # Process data in windows (prop_window_size=30) over entire dataset\n",
    "    prop_window_size = 30\n",
    "    num_windows = df.shape[0] // prop_window_size\n",
    "    max_triplets = 10\n",
    "    all_window_avgs = []\n",
    "    \n",
    "    def process_window_prop(w):\n",
    "        window_df = df.iloc[w*prop_window_size : (w+1)*prop_window_size]\n",
    "        pair_delays = {}\n",
    "        for triplet in islice(combinations(stocks, 3), max_triplets):\n",
    "            data = window_df[list(triplet)].values  # shape: (prop_window_size, 3)\n",
    "            n_components_local = 3\n",
    "            ica_local = FastICA(n_components=n_components_local, random_state=0, max_iter=2000, tol=1e-3)\n",
    "            try:\n",
    "                S = ica_local.fit_transform(data)\n",
    "            except Exception:\n",
    "                continue\n",
    "            A = ica_local.mixing_\n",
    "            best_comp = None\n",
    "            best_spread = np.inf\n",
    "            for k in range(n_components_local):\n",
    "                spread = np.max(A[:, k]) - np.min(A[:, k])\n",
    "                if spread < best_spread:\n",
    "                    best_spread = spread\n",
    "                    best_comp = k\n",
    "            rec_signals = {}\n",
    "            for idx, stock in enumerate(triplet):\n",
    "                rec_signal = A[idx, best_comp] * S[:, best_comp]\n",
    "                rec_signals[stock] = rec_signal\n",
    "            for stock1, stock2 in combinations(triplet, 2):\n",
    "                delay_val = avg_time_delay(rec_signals[stock1], rec_signals[stock2])\n",
    "                if np.isclose(delay_val, 0.0):\n",
    "                    continue\n",
    "                if delay_val > 0:\n",
    "                    key = (stock1, stock2)\n",
    "                    val = delay_val\n",
    "                else:\n",
    "                    key = (stock2, stock1)\n",
    "                    val = -delay_val\n",
    "                pair_delays.setdefault(key, []).append(val)\n",
    "        window_avg = {pair: np.mean(vals) for pair, vals in pair_delays.items()}\n",
    "        return window_avg\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = {executor.submit(process_window_prop, w): w for w in range(num_windows)}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_window_avgs.append(result)\n",
    "            except Exception as e:\n",
    "                print(\"A window failed:\", e)\n",
    "    \n",
    "    # Aggregate delays over windows\n",
    "    aggregate_delays = {}\n",
    "    for window_avg in all_window_avgs:\n",
    "        for pair, delay_val in window_avg.items():\n",
    "            if pair in aggregate_delays:\n",
    "                s, cnt = aggregate_delays[pair]\n",
    "                aggregate_delays[pair] = (s + delay_val, cnt + 1)\n",
    "            else:\n",
    "                aggregate_delays[pair] = (delay_val, 1)\n",
    "    cumulative_avg = {pair: s/cnt for pair, (s, cnt) in aggregate_delays.items()}\n",
    "    \n",
    "    # Build propagation graph dictionary from cumulative delays\n",
    "    prop_graph = {stock: [] for stock in stocks}\n",
    "    for (u, v), avg_delay in cumulative_avg.items():\n",
    "        prop_graph.setdefault(u, []).append(v)\n",
    "    \n",
    "    # Draw and save the final propagation graph and delays CSV\n",
    "    output_folder = \"graphs\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    final_graph_file = os.path.join(output_folder, \"final_cumulative_prop_graph.png\")\n",
    "    draw_graph(prop_graph, cumulative_avg, title=\"Final Cumulative Propagation Graph\", filename=final_graph_file)\n",
    "    print(f\"Saved final propagation graph to {final_graph_file}\")\n",
    "    final_df = pd.DataFrame([(u, v, avg_delay) for (u, v), avg_delay in cumulative_avg.items()],\n",
    "                            columns=[\"Stock1\", \"Stock2\", \"AvgDelay\"])\n",
    "    final_df.to_csv(os.path.join(output_folder, \"final_cumulative_delays.csv\"), index=False)\n",
    "    print(\"Saved cumulative propagation delays CSV.\")\n",
    "    \n",
    "    # Convert propagation graph delays into edge tensors.\n",
    "    def convert_prop_graph_to_edge_tensors(delays):\n",
    "        edges = []\n",
    "        weights = []\n",
    "        for (u, v), delay in delays.items():\n",
    "            edges.append([stocks.index(u), stocks.index(v)])\n",
    "            weights.append(np.exp(-delay))\n",
    "        edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "        edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    edge_index_prop, edge_weight_prop = convert_prop_graph_to_edge_tensors(cumulative_avg)\n",
    "    print(\"Propagation graph edges (tensor):\", edge_index_prop.shape[1])\n",
    "    \n",
    "    # Create datasets using the multi-feature DataFrame\n",
    "    window_size = 60\n",
    "    train_dataset = StockFeatureDataset(train_features, window_size)\n",
    "    test_dataset = StockFeatureDataset(test_features, window_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Input dimension = window_size * number of features (3)\n",
    "    input_dim = window_size * 3\n",
    "    model = StockKAN(input_dim=input_dim, hidden_dim=64, output_dim=1, heads=4)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Set forecast horizon for 2 years (approx. 730 trading days)\n",
    "    forecast_horizon = 730\n",
    "    epochs = 50\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X, y in train_loader:\n",
    "            batch_loss = 0.0\n",
    "            for i in range(X.shape[0]):\n",
    "                x_sample = X[i].to(device)  # shape: (num_stocks, input_dim)\n",
    "                target = y[i].unsqueeze(1).to(device)  # shape: (num_stocks, 1)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(x_sample, edge_index_prop.to(device), edge_weight_prop.to(device))\n",
    "                loss = criterion(out, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss += loss.item()\n",
    "            epoch_loss += batch_loss / X.shape[0]\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.6f}\")\n",
    "    \n",
    "    # Evaluate on the test set\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            for i in range(X.shape[0]):\n",
    "                x_sample = X[i].to(device)\n",
    "                target = y[i].unsqueeze(1).to(device)\n",
    "                out = model(x_sample, edge_index_prop.to(device), edge_weight_prop.to(device))\n",
    "                predictions.append(out.squeeze().cpu().numpy())\n",
    "                actuals.append(target.squeeze().cpu().numpy())\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    print(f\"Test RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    pred_sign = np.sign(predictions)\n",
    "    actual_sign = np.sign(actuals)\n",
    "    binary_accuracy = np.mean(pred_sign == actual_sign)\n",
    "    print(f\"Test Binary Accuracy: {binary_accuracy:.6f}\")\n",
    "    \n",
    "    # Plot predicted vs actual returns for Stock 0 (2 Years)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(predictions[:, 0], label=\"Predicted\")\n",
    "    plt.plot(actuals[:, 0], label=\"Actual\")\n",
    "    plt.xlabel(\"Test Sample Index\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.title(\"Predicted vs Actual Returns for Stock 0 (2 Years)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # ----------------- PnL Chart for Stock 0 (2 Years) -----------------\n",
    "    positions = np.sign(predictions[:, 0])\n",
    "    daily_pnl = positions * actuals[:, 0]\n",
    "    cumulative_pnl = np.cumsum(daily_pnl)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cumulative_pnl, marker='o', linestyle='-', label=\"Cumulative PnL\")\n",
    "    plt.xlabel(\"Test Sample Index (Days)\")\n",
    "    plt.ylabel(\"Cumulative PnL\")\n",
    "    plt.title(\"PnL Chart for Stock 0 (2 Years)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0967422e-b2cf-405b-b009-fb73f58c95ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in log\n",
      "  result = func(self.values, **kwargs)\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:127: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 465\u001b[0m\n\u001b[1;32m    463\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 321\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m    320\u001b[0m     futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(process_window_prop, w): w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_windows)}\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m             result \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    248\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations, islice\n",
    "from sklearn.decomposition import FastICA\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from numba import njit\n",
    "import cupy as cp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Workaround for torch_geometric compatibility ---\n",
    "if not hasattr(torch.serialization, 'add_safe_globals'):\n",
    "    torch.serialization.add_safe_globals = lambda x: None\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ===================== Propagation Graph Implementation =====================\n",
    "@njit\n",
    "def cost(a, b):\n",
    "    return abs(a - b)\n",
    "\n",
    "@njit\n",
    "def compute_d_matrix(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = np.empty((T, T), dtype=np.float64)\n",
    "    D[0, 0] = cost(s[0], t[0])\n",
    "    for j in range(1, T):\n",
    "        D[0, j] = D[0, j-1] + cost(s[0], t[j])\n",
    "    for i in range(1, T):\n",
    "        D[i, 0] = D[i-1, 0] + cost(s[i], t[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            a_val = D[i-1, j]\n",
    "            b_val = D[i, j-1]\n",
    "            c_val = D[i-1, j-1]\n",
    "            D[i, j] = min(a_val, b_val, c_val) + cost(s[i], t[j])\n",
    "    return D\n",
    "\n",
    "@njit\n",
    "def compute_F_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    F = np.zeros((T, T), dtype=np.float64)\n",
    "    F[0, 0] = 1.0\n",
    "    for i in range(T):\n",
    "        for j in range(T):\n",
    "            if i == 0 and j == 0:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            c = cost(s[i], t[j])\n",
    "            if i > 0 and abs(D[i, j] - (D[i-1, j] + c)) < 1e-8:\n",
    "                total += F[i-1, j]\n",
    "            if j > 0 and abs(D[i, j] - (D[i, j-1] + c)) < 1e-8:\n",
    "                total += F[i, j-1]\n",
    "            if i > 0 and j > 0 and abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total += F[i-1, j-1]\n",
    "            F[i, j] = total\n",
    "    return F\n",
    "\n",
    "@njit\n",
    "def compute_B_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    B = np.zeros((T, T), dtype=np.float64)\n",
    "    B[T-1, T-1] = 1.0\n",
    "    for i in range(T-1, -1, -1):\n",
    "        for j in range(T-1, -1, -1):\n",
    "            if i == T-1 and j == T-1:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            if i+1 < T and abs(D[i+1, j] - (D[i, j] + cost(s[i+1], t[j]))) < 1e-8:\n",
    "                total += B[i+1, j]\n",
    "            if j+1 < T and abs(D[i, j+1] - (D[i, j] + cost(s[i], t[j+1]))) < 1e-8:\n",
    "                total += B[i, j+1]\n",
    "            if i+1 < T and j+1 < T and abs(D[i+1, j+1] - (D[i, j] + cost(s[i+1], t[j+1]))) < 1e-8:\n",
    "                total += B[i+1, j+1]\n",
    "            B[i, j] = total\n",
    "    return B\n",
    "\n",
    "@njit\n",
    "def avg_time_delay(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = compute_d_matrix(s, t)\n",
    "    F = compute_F_matrix(s, t, D)\n",
    "    B = compute_B_matrix(s, t, D)\n",
    "    total_delay = 0.0\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            c = cost(s[i], t[j])\n",
    "            if abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total_delay += (j - i) * F[i-1, j-1] * B[i, j]\n",
    "    num_alignments = B[0, 0]\n",
    "    if num_alignments == 0:\n",
    "        return 0.0\n",
    "    return total_delay / num_alignments\n",
    "\n",
    "def avg_time_delay_gpu(s, t):\n",
    "    s_gpu = cp.asarray(s)\n",
    "    t_gpu = cp.asarray(t)\n",
    "    T = s_gpu.shape[0]\n",
    "    D_gpu = cp.zeros((T, T), dtype=cp.float64)\n",
    "    D_gpu[0, 0] = cp.abs(s_gpu[0] - t_gpu[0])\n",
    "    for j in range(1, T):\n",
    "        D_gpu[0, j] = D_gpu[0, j-1] + cp.abs(s_gpu[0] - t_gpu[j])\n",
    "    for i in range(1, T):\n",
    "        D_gpu[i, 0] = D_gpu[i-1, 0] + cp.abs(s_gpu[i] - t_gpu[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            D_gpu[i, j] = cp.minimum(cp.minimum(D_gpu[i-1, j], D_gpu[i, j-1]),\n",
    "                                       D_gpu[i-1, j-1]) + cp.abs(s_gpu[i] - t_gpu[j])\n",
    "    D = cp.asnumpy(D_gpu)\n",
    "    return avg_time_delay(s, t)\n",
    "\n",
    "def build_propagation_graph(signal_list):\n",
    "    \"\"\"\n",
    "    Construct a propagation graph from a list of stock signals.\n",
    "    Each signal is a 1D numpy array.\n",
    "    Returns:\n",
    "        graph: dict mapping stock index to list of neighbor indices.\n",
    "        delays: dict mapping (i, j) to average delay.\n",
    "    \"\"\"\n",
    "    N = len(signal_list)\n",
    "    edges = []\n",
    "    delays = {}\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            delay_val = avg_time_delay(np.array(signal_list[i]), np.array(signal_list[j]))\n",
    "            if np.isclose(delay_val, 0.0):\n",
    "                continue\n",
    "            if delay_val > 0:\n",
    "                edges.append((i, j))\n",
    "                delays[(i, j)] = delay_val\n",
    "            else:\n",
    "                edges.append((j, i))\n",
    "                delays[(j, i)] = -delay_val\n",
    "    graph = {}\n",
    "    for (u, v) in edges:\n",
    "        graph.setdefault(u, []).append(v)\n",
    "    return graph, delays\n",
    "\n",
    "def draw_graph(graph, delays, title=\"Propagation Graph\", filename=\"graph.png\"):\n",
    "    G = nx.DiGraph()\n",
    "    for u, vs in graph.items():\n",
    "        for v in vs:\n",
    "            G.add_edge(u, v, weight=delays.get((u, v), 0))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", arrowstyle=\"->\", arrowsize=20)\n",
    "    edge_labels = {(u, v): f\"{data['weight']:.2f}\" for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def convert_prop_graph_to_edge_tensors(delays):\n",
    "    \"\"\"\n",
    "    Convert the delays dictionary into edge_index and edge_weight tensors.\n",
    "    We use exp(-delay) as weight.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    weights = []\n",
    "    for (u, v), delay in delays.items():\n",
    "        edges.append([u, v])\n",
    "        weights.append(np.exp(-delay))\n",
    "    edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "    edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "# ===================== Feature Engineering & Dataset =====================\n",
    "def compute_features(prices, window=5):\n",
    "    \"\"\"\n",
    "    Compute log return, rolling mean, and rolling std.\n",
    "    Returns a DataFrame with MultiIndex columns (stock, feature).\n",
    "    \"\"\"\n",
    "    log_ret = np.log(prices) - np.log(prices.shift(1))\n",
    "    rolling_mean = log_ret.rolling(window=window).mean()\n",
    "    rolling_std = log_ret.rolling(window=window).std()\n",
    "    \n",
    "    arrays = []\n",
    "    for stock in prices.columns:\n",
    "        for feat in ['log', 'mean', 'std']:\n",
    "            arrays.append((stock, feat))\n",
    "    col_index = pd.MultiIndex.from_tuples(arrays, names=['stock', 'feature'])\n",
    "    feature_df = pd.DataFrame(index=prices.index, columns=col_index)\n",
    "    for stock in prices.columns:\n",
    "        feature_df[(stock, 'log')] = log_ret[stock]\n",
    "        feature_df[(stock, 'mean')] = rolling_mean[stock]\n",
    "        feature_df[(stock, 'std')] = rolling_std[stock]\n",
    "    feature_df = feature_df.dropna()\n",
    "    return feature_df\n",
    "\n",
    "class StockFeatureDataset(Dataset):\n",
    "    def __init__(self, feature_df, window_size):\n",
    "        \"\"\"\n",
    "        feature_df: DataFrame with MultiIndex columns (stock, feature)\n",
    "        window_size: number of historical days.\n",
    "        For each sample, for each stock, extract a window of features (shape: (window_size, num_features)),\n",
    "        flatten it, and use as input.\n",
    "        Target: next-day log return for each stock.\n",
    "        \"\"\"\n",
    "        self.feature_df = feature_df\n",
    "        self.window_size = window_size\n",
    "        self.stocks = feature_df.columns.levels[0].tolist()\n",
    "        self.features = feature_df.columns.levels[1].tolist()\n",
    "        self.num_features = len(self.features)\n",
    "        self.num_stocks = len(self.stocks)\n",
    "        self.num_samples = feature_df.shape[0] - window_size\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, idx):\n",
    "        X_list = []\n",
    "        targets = []\n",
    "        for stock in self.stocks:\n",
    "            stock_data = self.feature_df[stock].iloc[idx: idx + self.window_size].values\n",
    "            X_list.append(stock_data.flatten())\n",
    "            targets.append(self.feature_df[stock].iloc[idx + self.window_size]['log'])\n",
    "        X = np.vstack(X_list)\n",
    "        y = np.array(targets)\n",
    "        return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# ===================== KAN Model Definition =====================\n",
    "class StockKAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "        super(StockKAN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gat = GATConv(hidden_dim, hidden_dim, heads=heads, concat=False)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.encoder(x)\n",
    "        x = self.gat(x, edge_index)\n",
    "        out = self.predictor(x)\n",
    "        return out\n",
    "\n",
    "# ===================== Main Pipeline =====================\n",
    "def main():\n",
    "    # Load price data\n",
    "    df = pd.read_csv('./data/global_titans.csv')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    stocks = df.columns.tolist()\n",
    "    \n",
    "    # Compute features: log return, rolling mean, and rolling std (window=5)\n",
    "    features_df = compute_features(df, window=5)\n",
    "    \n",
    "    # Split into training and testing sets (first half for training, second half for testing)\n",
    "    T = features_df.shape[0]\n",
    "    train_features = features_df.iloc[:T//2]\n",
    "    test_features = features_df.iloc[T//2:]\n",
    "    \n",
    "    # For propagation graph, use training log returns (from features)\n",
    "    train_returns = train_features.xs('log', level='feature', axis=1)\n",
    "    train_data = train_returns.values\n",
    "    n_components = min(3, train_data.shape[1])\n",
    "    ica = FastICA(n_components=n_components, random_state=0, max_iter=5000, tol=1e-3)\n",
    "    try:\n",
    "        ica.fit(train_data)\n",
    "    except Exception as e:\n",
    "        print(\"FastICA failed on training data:\", e)\n",
    "        return\n",
    "    \n",
    "    # Build propagation graph using the provided implementation:\n",
    "    # Process data in windows (prop_window_size=30) over entire dataset\n",
    "    prop_window_size = 30\n",
    "    num_windows = df.shape[0] // prop_window_size\n",
    "    max_triplets = 10\n",
    "    all_window_avgs = []\n",
    "    \n",
    "    def process_window_prop(w):\n",
    "        window_df = df.iloc[w*prop_window_size : (w+1)*prop_window_size]\n",
    "        pair_delays = {}\n",
    "        for triplet in islice(combinations(stocks, 3), max_triplets):\n",
    "            data = window_df[list(triplet)].values  # shape: (prop_window_size, 3)\n",
    "            n_components_local = 3\n",
    "            ica_local = FastICA(n_components=n_components_local, random_state=0, max_iter=2000, tol=1e-3)\n",
    "            try:\n",
    "                S = ica_local.fit_transform(data)\n",
    "            except Exception:\n",
    "                continue\n",
    "            A = ica_local.mixing_\n",
    "            best_comp = None\n",
    "            best_spread = np.inf\n",
    "            for k in range(n_components_local):\n",
    "                spread = np.max(A[:, k]) - np.min(A[:, k])\n",
    "                if spread < best_spread:\n",
    "                    best_spread = spread\n",
    "                    best_comp = k\n",
    "            rec_signals = {}\n",
    "            for idx, stock in enumerate(triplet):\n",
    "                rec_signal = A[idx, best_comp] * S[:, best_comp]\n",
    "                rec_signals[stock] = rec_signal\n",
    "            for stock1, stock2 in combinations(triplet, 2):\n",
    "                delay_val = avg_time_delay(rec_signals[stock1], rec_signals[stock2])\n",
    "                if np.isclose(delay_val, 0.0):\n",
    "                    continue\n",
    "                if delay_val > 0:\n",
    "                    key = (stock1, stock2)\n",
    "                    val = delay_val\n",
    "                else:\n",
    "                    key = (stock2, stock1)\n",
    "                    val = -delay_val\n",
    "                pair_delays.setdefault(key, []).append(val)\n",
    "        window_avg = {pair: np.mean(vals) for pair, vals in pair_delays.items()}\n",
    "        return window_avg\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = {executor.submit(process_window_prop, w): w for w in range(num_windows)}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_window_avgs.append(result)\n",
    "            except Exception as e:\n",
    "                print(\"A window failed:\", e)\n",
    "    \n",
    "    # Aggregate delays over windows\n",
    "    aggregate_delays = {}\n",
    "    for window_avg in all_window_avgs:\n",
    "        for pair, delay_val in window_avg.items():\n",
    "            if pair in aggregate_delays:\n",
    "                s, cnt = aggregate_delays[pair]\n",
    "                aggregate_delays[pair] = (s + delay_val, cnt + 1)\n",
    "            else:\n",
    "                aggregate_delays[pair] = (delay_val, 1)\n",
    "    cumulative_avg = {pair: s/cnt for pair, (s, cnt) in aggregate_delays.items()}\n",
    "    \n",
    "    # Build propagation graph dictionary from cumulative delays\n",
    "    prop_graph = {stock: [] for stock in stocks}\n",
    "    for (u, v), avg_delay in cumulative_avg.items():\n",
    "        prop_graph.setdefault(u, []).append(v)\n",
    "    \n",
    "    # Draw and save the final propagation graph and delays CSV\n",
    "    output_folder = \"graphs\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    final_graph_file = os.path.join(output_folder, \"final_cumulative_prop_graph.png\")\n",
    "    draw_graph(prop_graph, cumulative_avg, title=\"Final Cumulative Propagation Graph\", filename=final_graph_file)\n",
    "    print(f\"Saved final propagation graph to {final_graph_file}\")\n",
    "    final_df = pd.DataFrame([(u, v, avg_delay) for (u, v), avg_delay in cumulative_avg.items()],\n",
    "                            columns=[\"Stock1\", \"Stock2\", \"AvgDelay\"])\n",
    "    final_df.to_csv(os.path.join(output_folder, \"final_cumulative_delays.csv\"), index=False)\n",
    "    print(\"Saved cumulative propagation delays CSV.\")\n",
    "    \n",
    "    # Convert propagation graph delays into edge tensors.\n",
    "    def convert_prop_graph_to_edge_tensors(delays):\n",
    "        edges = []\n",
    "        weights = []\n",
    "        for (u, v), delay in delays.items():\n",
    "            edges.append([stocks.index(u), stocks.index(v)])\n",
    "            weights.append(np.exp(-delay))\n",
    "        edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "        edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    edge_index_prop, edge_weight_prop = convert_prop_graph_to_edge_tensors(cumulative_avg)\n",
    "    print(\"Propagation graph edges (tensor):\", edge_index_prop.shape[1])\n",
    "    \n",
    "    # Create datasets using the multi-feature DataFrame\n",
    "    window_size = 60\n",
    "    train_dataset = StockFeatureDataset(train_features, window_size)\n",
    "    test_dataset = StockFeatureDataset(test_features, window_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    # Input dimension = window_size * number of features (3)\n",
    "    input_dim = window_size * 3\n",
    "    model = StockKAN(input_dim=input_dim, hidden_dim=64, output_dim=1, heads=4)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # --- Training improvements for speed:\n",
    "    # 1. Process entire batches at once (avoid per-sample loop).\n",
    "    # 2. Move edge tensors to the device outside the loop.\n",
    "    edge_index_prop = edge_index_prop.to(device)\n",
    "    edge_weight_prop = edge_weight_prop.to(device)\n",
    "    \n",
    "    forecast_horizon = 730  # 2 years ~730 trading days\n",
    "    epochs = 50\n",
    "    model.train()\n",
    "        \n",
    "    # Initialize AMP GradScaler for mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                batch_size, num_stocks, input_dim = X.shape\n",
    "                X_reshaped = X.view(-1, input_dim)\n",
    "                out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "                out = out.view(batch_size, num_stocks, -1)\n",
    "                loss = criterion(out.squeeze(), y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.6f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size, num_stocks, input_dim = X.shape\n",
    "            X_reshaped = X.view(-1, input_dim)\n",
    "            out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "            out = out.view(batch_size, num_stocks, -1)\n",
    "            predictions.append(out.squeeze().cpu().numpy())\n",
    "            actuals.append(y.cpu().numpy())\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    print(f\"Test RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    pred_sign = np.sign(predictions)\n",
    "    actual_sign = np.sign(actuals)\n",
    "    binary_accuracy = np.mean(pred_sign == actual_sign)\n",
    "    print(f\"Test Binary Accuracy: {binary_accuracy:.6f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(predictions[:, 0], label=\"Predicted\")\n",
    "    plt.plot(actuals[:, 0], label=\"Actual\")\n",
    "    plt.xlabel(\"Test Sample Index\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.title(\"Predicted vs Actual Returns for Stock 0 (2 Years)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    positions = np.sign(predictions[:, 0])\n",
    "    daily_pnl = positions * actuals[:, 0]\n",
    "    cumulative_pnl = np.cumsum(daily_pnl)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cumulative_pnl, marker='o', linestyle='-', label=\"Cumulative PnL\")\n",
    "    plt.xlabel(\"Test Sample Index (Days)\")\n",
    "    plt.ylabel(\"Cumulative PnL\")\n",
    "    plt.title(\"PnL Chart for Stock 0 (2 Years)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    torch.save(model, 'model.pth')\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc45af1b-da33-4a93-be7f-c8147d6b0acc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f88fd2-bddb-49ff-b999-3bf998fe061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations, islice\n",
    "from sklearn.decomposition import FastICA\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from numba import njit\n",
    "import cupy as cp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Workaround for torch_geometric compatibility ---\n",
    "if not hasattr(torch.serialization, 'add_safe_globals'):\n",
    "    torch.serialization.add_safe_globals = lambda x: None\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ===================== Propagation Graph Implementation =====================\n",
    "@njit\n",
    "def cost(a, b):\n",
    "    return abs(a - b)\n",
    "\n",
    "@njit\n",
    "def compute_d_matrix(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = np.empty((T, T), dtype=np.float64)\n",
    "    D[0, 0] = cost(s[0], t[0])\n",
    "    for j in range(1, T):\n",
    "        D[0, j] = D[0, j-1] + cost(s[0], t[j])\n",
    "    for i in range(1, T):\n",
    "        D[i, 0] = D[i-1, 0] + cost(s[i], t[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            a_val = D[i-1, j]\n",
    "            b_val = D[i, j-1]\n",
    "            c_val = D[i-1, j-1]\n",
    "            D[i, j] = min(a_val, b_val, c_val) + cost(s[i], t[j])\n",
    "    return D\n",
    "\n",
    "@njit\n",
    "def compute_F_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    F = np.zeros((T, T), dtype=np.float64)\n",
    "    F[0, 0] = 1.0\n",
    "    for i in range(T):\n",
    "        for j in range(T):\n",
    "            if i == 0 and j == 0:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            c = cost(s[i], t[j])\n",
    "            if i > 0 and abs(D[i, j] - (D[i-1, j] + c)) < 1e-8:\n",
    "                total += F[i-1, j]\n",
    "            if j > 0 and abs(D[i, j] - (D[i, j-1] + c)) < 1e-8:\n",
    "                total += F[i, j-1]\n",
    "            if i > 0 and j > 0 and abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total += F[i-1, j-1]\n",
    "            F[i, j] = total\n",
    "    return F\n",
    "\n",
    "@njit\n",
    "def compute_B_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    B = np.zeros((T, T), dtype=np.float64)\n",
    "    B[T-1, T-1] = 1.0\n",
    "    for i in range(T-1, -1, -1):\n",
    "        for j in range(T-1, -1, -1):\n",
    "            if i == T-1 and j == T-1:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            if i+1 < T and abs(D[i+1, j] - (D[i, j] + cost(s[i+1], t[j]))) < 1e-8:\n",
    "                total += B[i+1, j]\n",
    "            if j+1 < T and abs(D[i, j+1] - (D[i, j] + cost(s[i], t[j+1]))) < 1e-8:\n",
    "                total += B[i, j+1]\n",
    "            if i+1 < T and j+1 < T and abs(D[i+1, j+1] - (D[i, j] + cost(s[i+1], t[j+1]))) < 1e-8:\n",
    "                total += B[i+1, j+1]\n",
    "            B[i, j] = total\n",
    "    return B\n",
    "\n",
    "@njit\n",
    "def avg_time_delay(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = compute_d_matrix(s, t)\n",
    "    F = compute_F_matrix(s, t, D)\n",
    "    B = compute_B_matrix(s, t, D)\n",
    "    total_delay = 0.0\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            c = cost(s[i], t[j])\n",
    "            if abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total_delay += (j - i) * F[i-1, j-1] * B[i, j]\n",
    "    num_alignments = B[0, 0]\n",
    "    if num_alignments == 0:\n",
    "        return 0.0\n",
    "    return total_delay / num_alignments\n",
    "\n",
    "def avg_time_delay_gpu(s, t):\n",
    "    s_gpu = cp.asarray(s)\n",
    "    t_gpu = cp.asarray(t)\n",
    "    T = s_gpu.shape[0]\n",
    "    D_gpu = cp.zeros((T, T), dtype=cp.float64)\n",
    "    D_gpu[0, 0] = cp.abs(s_gpu[0] - t_gpu[0])\n",
    "    for j in range(1, T):\n",
    "        D_gpu[0, j] = D_gpu[0, j-1] + cp.abs(s_gpu[0] - t_gpu[j])\n",
    "    for i in range(1, T):\n",
    "        D_gpu[i, 0] = D_gpu[i-1, 0] + cp.abs(s_gpu[i] - t_gpu[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            D_gpu[i, j] = cp.minimum(cp.minimum(D_gpu[i-1, j], D_gpu[i, j-1]),\n",
    "                                       D_gpu[i-1, j-1]) + cp.abs(s_gpu[i] - t_gpu[j])\n",
    "    D = cp.asnumpy(D_gpu)\n",
    "    return avg_time_delay(s, t)\n",
    "\n",
    "def build_propagation_graph(signal_list):\n",
    "    \"\"\"\n",
    "    Construct a propagation graph from a list of stock signals.\n",
    "    Each signal is a 1D numpy array.\n",
    "    Returns:\n",
    "        graph: dict mapping stock index to list of neighbor indices.\n",
    "        delays: dict mapping (i, j) to average delay.\n",
    "    \"\"\"\n",
    "    N = len(signal_list)\n",
    "    edges = []\n",
    "    delays = {}\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            delay_val = avg_time_delay(np.array(signal_list[i]), np.array(signal_list[j]))\n",
    "            if np.isclose(delay_val, 0.0):\n",
    "                continue\n",
    "            if delay_val > 0:\n",
    "                edges.append((i, j))\n",
    "                delays[(i, j)] = delay_val\n",
    "            else:\n",
    "                edges.append((j, i))\n",
    "                delays[(j, i)] = -delay_val\n",
    "    graph = {}\n",
    "    for (u, v) in edges:\n",
    "        graph.setdefault(u, []).append(v)\n",
    "    return graph, delays\n",
    "\n",
    "def draw_graph(graph, delays, title=\"Propagation Graph\", filename=\"graph.png\"):\n",
    "    G = nx.DiGraph()\n",
    "    for u, vs in graph.items():\n",
    "        for v in vs:\n",
    "            G.add_edge(u, v, weight=delays.get((u, v), 0))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", arrowstyle=\"->\", arrowsize=20)\n",
    "    edge_labels = {(u, v): f\"{data['weight']:.2f}\" for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def convert_prop_graph_to_edge_tensors(delays):\n",
    "    \"\"\"\n",
    "    Convert the delays dictionary into edge_index and edge_weight tensors.\n",
    "    We use exp(-delay) as weight.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    weights = []\n",
    "    for (u, v), delay in delays.items():\n",
    "        edges.append([u, v])\n",
    "        weights.append(np.exp(-delay))\n",
    "    edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "    edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "# ===================== Feature Engineering & Dataset =====================\n",
    "def compute_features(prices, window=5):\n",
    "    \"\"\"\n",
    "    Compute log return, rolling mean, and rolling std.\n",
    "    Returns a DataFrame with MultiIndex columns (stock, feature).\n",
    "    \"\"\"\n",
    "    log_ret = np.log(prices) - np.log(prices.shift(1))\n",
    "    rolling_mean = log_ret.rolling(window=window).mean()\n",
    "    rolling_std = log_ret.rolling(window=window).std()\n",
    "    \n",
    "    arrays = []\n",
    "    for stock in prices.columns:\n",
    "        for feat in ['log', 'mean', 'std']:\n",
    "            arrays.append((stock, feat))\n",
    "    col_index = pd.MultiIndex.from_tuples(arrays, names=['stock', 'feature'])\n",
    "    feature_df = pd.DataFrame(index=prices.index, columns=col_index)\n",
    "    for stock in prices.columns:\n",
    "        feature_df[(stock, 'log')] = log_ret[stock]\n",
    "        feature_df[(stock, 'mean')] = rolling_mean[stock]\n",
    "        feature_df[(stock, 'std')] = rolling_std[stock]\n",
    "    feature_df = feature_df.dropna()\n",
    "    return feature_df\n",
    "\n",
    "class StockFeatureDataset(Dataset):\n",
    "    def __init__(self, feature_df, window_size):\n",
    "        \"\"\"\n",
    "        feature_df: DataFrame with MultiIndex columns (stock, feature)\n",
    "        window_size: number of historical days.\n",
    "        For each sample, for each stock, extract a window of features (shape: (window_size, num_features)),\n",
    "        flatten it, and use as input.\n",
    "        Target: next-day log return for each stock.\n",
    "        \"\"\"\n",
    "        self.feature_df = feature_df\n",
    "        self.window_size = window_size\n",
    "        self.stocks = feature_df.columns.levels[0].tolist()\n",
    "        self.features = feature_df.columns.levels[1].tolist()\n",
    "        self.num_features = len(self.features)\n",
    "        self.num_stocks = len(self.stocks)\n",
    "        self.num_samples = feature_df.shape[0] - window_size\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, idx):\n",
    "        X_list = []\n",
    "        targets = []\n",
    "        for stock in self.stocks:\n",
    "            stock_data = self.feature_df[stock].iloc[idx: idx + self.window_size].values\n",
    "            X_list.append(stock_data.flatten())\n",
    "            targets.append(self.feature_df[stock].iloc[idx + self.window_size]['log'])\n",
    "        X = np.vstack(X_list)\n",
    "        y = np.array(targets)\n",
    "        return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# ===================== KAN Model Definition =====================\n",
    "class StockKAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "        super(StockKAN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gat = GATConv(hidden_dim, hidden_dim, heads=heads, concat=False)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.encoder(x)\n",
    "        x = self.gat(x, edge_index)\n",
    "        out = self.predictor(x)\n",
    "        return out\n",
    "\n",
    "# ===================== Main Pipeline =====================\n",
    "def main():\n",
    "    # Load price data\n",
    "    df = pd.read_csv('./data/global_titans.csv')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    stocks = df.columns.tolist()\n",
    "    \n",
    "    # Compute features: log return, rolling mean, and rolling std (window=5)\n",
    "    features_df = compute_features(df, window=5)\n",
    "    \n",
    "    # Split into training and testing sets (first half for training, second half for testing)\n",
    "    T = features_df.shape[0]\n",
    "    train_features = features_df.iloc[:T//2]\n",
    "    test_features = features_df.iloc[T//2:]\n",
    "    \n",
    "    # For propagation graph, use training log returns (from features)\n",
    "    train_returns = train_features.xs('log', level='feature', axis=1)\n",
    "    train_data = train_returns.values\n",
    "    n_components = min(3, train_data.shape[1])\n",
    "    ica = FastICA(n_components=n_components, random_state=0, max_iter=5000, tol=1e-3)\n",
    "    try:\n",
    "        ica.fit(train_data)\n",
    "    except Exception as e:\n",
    "        print(\"FastICA failed on training data:\", e)\n",
    "        return\n",
    "    \n",
    "    # Build propagation graph using the provided implementation:\n",
    "    # Process data in windows (prop_window_size=30) over entire dataset\n",
    "    prop_window_size = 30\n",
    "    num_windows = df.shape[0] // prop_window_size\n",
    "    max_triplets = 10\n",
    "    all_window_avgs = []\n",
    "    \n",
    "    def process_window_prop(w):\n",
    "        window_df = df.iloc[w*prop_window_size : (w+1)*prop_window_size]\n",
    "        pair_delays = {}\n",
    "        for triplet in islice(combinations(stocks, 3), max_triplets):\n",
    "            data = window_df[list(triplet)].values  # shape: (prop_window_size, 3)\n",
    "            n_components_local = 3\n",
    "            ica_local = FastICA(n_components=n_components_local, random_state=0, max_iter=5000, tol=1e-3)\n",
    "            try:\n",
    "                S = ica_local.fit_transform(data)\n",
    "            except Exception:\n",
    "                continue\n",
    "            A = ica_local.mixing_\n",
    "            best_comp = None\n",
    "            best_spread = np.inf\n",
    "            for k in range(n_components_local):\n",
    "                spread = np.max(A[:, k]) - np.min(A[:, k])\n",
    "                if spread < best_spread:\n",
    "                    best_spread = spread\n",
    "                    best_comp = k\n",
    "            rec_signals = {}\n",
    "            for idx, stock in enumerate(triplet):\n",
    "                rec_signal = A[idx, best_comp] * S[:, best_comp]\n",
    "                rec_signals[stock] = rec_signal\n",
    "            for stock1, stock2 in combinations(triplet, 2):\n",
    "                delay_val = avg_time_delay(rec_signals[stock1], rec_signals[stock2])\n",
    "                if np.isclose(delay_val, 0.0):\n",
    "                    continue\n",
    "                if delay_val > 0:\n",
    "                    key = (stock1, stock2)\n",
    "                    val = delay_val\n",
    "                else:\n",
    "                    key = (stock2, stock1)\n",
    "                    val = -delay_val\n",
    "                pair_delays.setdefault(key, []).append(val)\n",
    "        window_avg = {pair: np.mean(vals) for pair, vals in pair_delays.items()}\n",
    "        return window_avg\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = {executor.submit(process_window_prop, w): w for w in range(num_windows)}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_window_avgs.append(result)\n",
    "            except Exception as e:\n",
    "                print(\"A window failed:\", e)\n",
    "    \n",
    "    # Aggregate delays over windows\n",
    "    aggregate_delays = {}\n",
    "    for window_avg in all_window_avgs:\n",
    "        for pair, delay_val in window_avg.items():\n",
    "            if pair in aggregate_delays:\n",
    "                s, cnt = aggregate_delays[pair]\n",
    "                aggregate_delays[pair] = (s + delay_val, cnt + 1)\n",
    "            else:\n",
    "                aggregate_delays[pair] = (delay_val, 1)\n",
    "    cumulative_avg = {pair: s/cnt for pair, (s, cnt) in aggregate_delays.items()}\n",
    "    \n",
    "    # Build propagation graph dictionary from cumulative delays\n",
    "    prop_graph = {stock: [] for stock in stocks}\n",
    "    for (u, v), avg_delay in cumulative_avg.items():\n",
    "        prop_graph.setdefault(u, []).append(v)\n",
    "    \n",
    "    # Draw and save the final propagation graph and delays CSV\n",
    "    output_folder = \"graphs\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    final_graph_file = os.path.join(output_folder, \"final_cumulative_prop_graph.png\")\n",
    "    draw_graph(prop_graph, cumulative_avg, title=\"Final Cumulative Propagation Graph\", filename=final_graph_file)\n",
    "    print(f\"Saved final propagation graph to {final_graph_file}\")\n",
    "    final_df = pd.DataFrame([(u, v, avg_delay) for (u, v), avg_delay in cumulative_avg.items()],\n",
    "                            columns=[\"Stock1\", \"Stock2\", \"AvgDelay\"])\n",
    "    final_df.to_csv(os.path.join(output_folder, \"final_cumulative_delays.csv\"), index=False)\n",
    "    print(\"Saved cumulative propagation delays CSV.\")\n",
    "    \n",
    "    # Convert propagation graph delays into edge tensors.\n",
    "    def convert_prop_graph_to_edge_tensors(delays):\n",
    "        edges = []\n",
    "        weights = []\n",
    "        for (u, v), delay in delays.items():\n",
    "            edges.append([stocks.index(u), stocks.index(v)])\n",
    "            weights.append(np.exp(-delay))\n",
    "        edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "        edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    edge_index_prop, edge_weight_prop = convert_prop_graph_to_edge_tensors(cumulative_avg)\n",
    "    print(\"Propagation graph edges (tensor):\", edge_index_prop.shape[1])\n",
    "    \n",
    "    # Create datasets using the multi-feature DataFrame\n",
    "    window_size = 60\n",
    "    train_dataset = StockFeatureDataset(train_features, window_size)\n",
    "    test_dataset = StockFeatureDataset(test_features, window_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    # Input dimension = window_size * number of features (3)\n",
    "    input_dim = window_size * 3\n",
    "    model = StockKAN(input_dim=input_dim, hidden_dim=64, output_dim=1, heads=4)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # --- Training improvements for speed:\n",
    "    # 1. Process entire batches at once (avoid per-sample loop).\n",
    "    # 2. Move edge tensors to the device outside the loop.\n",
    "    edge_index_prop = edge_index_prop.to(device)\n",
    "    edge_weight_prop = edge_weight_prop.to(device)\n",
    "    \n",
    "    forecast_horizon = 2190  # 2 years ~730 trading days\n",
    "    epochs = 50\n",
    "    model.train()\n",
    "        \n",
    "    # Initialize AMP GradScaler for mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                batch_size, num_stocks, input_dim = X.shape\n",
    "                X_reshaped = X.view(-1, input_dim)\n",
    "                out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "                out = out.view(batch_size, num_stocks, -1)\n",
    "                loss = criterion(out.squeeze(), y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.6f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size, num_stocks, input_dim = X.shape\n",
    "            X_reshaped = X.view(-1, input_dim)\n",
    "            out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "            out = out.view(batch_size, num_stocks, -1)\n",
    "            predictions.append(out.squeeze().cpu().numpy())\n",
    "            actuals.append(y.cpu().numpy())\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    print(f\"Test RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    print(f\"Test R^2: {r2:.6f}\")\n",
    "    \n",
    "    pred_sign = np.sign(predictions)\n",
    "    actual_sign = np.sign(actuals)\n",
    "    binary_accuracy = np.mean(pred_sign == actual_sign)\n",
    "    print(f\"Test Binary Accuracy: {binary_accuracy:.6f}\")\n",
    "    \n",
    "    # Print stock names\n",
    "    print(\"Stocks:\", stocks)\n",
    "    \n",
    "    # Plot correlation graph (scatter plot) for first stock\n",
    "    stock_index = 0\n",
    "    stock_name = stocks[stock_index]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(actuals[:, stock_index], predictions[:, stock_index], alpha=0.6)\n",
    "    plt.xlabel(\"Actual Returns\")\n",
    "    plt.ylabel(\"Predicted Returns\")\n",
    "    plt.title(f\"Correlation Scatter for {stock_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, f\"correlation_{stock_name}.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Portfolio metrics on returns for first stock\n",
    "    returns = actuals[:, stock_index]\n",
    "    mean_return = np.mean(returns)\n",
    "    std_return = np.std(returns)\n",
    "    annualized_vol = std_return * np.sqrt(252)\n",
    "    sharpe_ratio = (mean_return / std_return) * np.sqrt(252) if std_return != 0 else 0.0\n",
    "    print(f\"Metrics for {stock_name}:\")\n",
    "    print(f\"  Mean Daily Return: {mean_return:.6f}\")\n",
    "    print(f\"  Daily Return Std Dev: {std_return:.6f}\")\n",
    "    print(f\"  Annualized Volatility: {annualized_vol:.6f}\")\n",
    "    print(f\"  Sharpe Ratio: {sharpe_ratio:.6f}\")\n",
    "    \n",
    "    # Plot predicted vs actual returns for first stock\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(predictions[:, stock_index], label=\"Predicted\")\n",
    "    plt.plot(actuals[:, stock_index], label=\"Actual\")\n",
    "    plt.xlabel(\"Test Sample Index\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.title(f\"Predicted vs Actual Returns for {stock_name} (2 Years)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot cumulative PnL for first stock\n",
    "    positions = np.sign(predictions[:, stock_index])\n",
    "    daily_pnl = positions * actuals[:, stock_index]\n",
    "    cumulative_pnl = np.cumsum(daily_pnl)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cumulative_pnl, marker='o', linestyle='-', label=\"Cumulative PnL\")\n",
    "    plt.xlabel(\"Test Sample Index (Days)\")\n",
    "    plt.ylabel(\"Cumulative PnL\")\n",
    "    plt.title(f\"PnL Chart for {stock_name} (2 Years)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa873e-9789-4b94-b351-60e126dfd5b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations, islice\n",
    "from sklearn.decomposition import FastICA\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from numba import njit\n",
    "import cupy as cp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Workaround for torch_geometric compatibility ---\n",
    "if not hasattr(torch.serialization, 'add_safe_globals'):\n",
    "    torch.serialization.add_safe_globals = lambda x: None\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ===================== Propagation Graph Implementation =====================\n",
    "@njit\n",
    "def cost(a, b):\n",
    "    return abs(a - b)\n",
    "\n",
    "@njit\n",
    "def compute_d_matrix(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = np.empty((T, T), dtype=np.float64)\n",
    "    D[0, 0] = cost(s[0], t[0])\n",
    "    for j in range(1, T):\n",
    "        D[0, j] = D[0, j-1] + cost(s[0], t[j])\n",
    "    for i in range(1, T):\n",
    "        D[i, 0] = D[i-1, 0] + cost(s[i], t[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            a_val = D[i-1, j]\n",
    "            b_val = D[i, j-1]\n",
    "            c_val = D[i-1, j-1]\n",
    "            D[i, j] = min(a_val, b_val, c_val) + cost(s[i], t[j])\n",
    "    return D\n",
    "\n",
    "@njit\n",
    "def compute_F_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    F = np.zeros((T, T), dtype=np.float64)\n",
    "    F[0, 0] = 1.0\n",
    "    for i in range(T):\n",
    "        for j in range(T):\n",
    "            if i == 0 and j == 0:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            c = cost(s[i], t[j])\n",
    "            if i > 0 and abs(D[i, j] - (D[i-1, j] + c)) < 1e-8:\n",
    "                total += F[i-1, j]\n",
    "            if j > 0 and abs(D[i, j] - (D[i, j-1] + c)) < 1e-8:\n",
    "                total += F[i, j-1]\n",
    "            if i > 0 and j > 0 and abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total += F[i-1, j-1]\n",
    "            F[i, j] = total\n",
    "    return F\n",
    "\n",
    "@njit\n",
    "def compute_B_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    B = np.zeros((T, T), dtype=np.float64)\n",
    "    B[T-1, T-1] = 1.0\n",
    "    for i in range(T-1, -1, -1):\n",
    "        for j in range(T-1, -1, -1):\n",
    "            if i == T-1 and j == T-1:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            if i+1 < T and abs(D[i+1, j] - (D[i, j] + cost(s[i+1], t[j]))) < 1e-8:\n",
    "                total += B[i+1, j]\n",
    "            if j+1 < T and abs(D[i, j+1] - (D[i, j] + cost(s[i], t[j+1]))) < 1e-8:\n",
    "                total += B[i, j+1]\n",
    "            if i+1 < T and j+1 < T and abs(D[i+1, j+1] - (D[i, j] + cost(s[i+1], t[j+1]))) < 1e-8:\n",
    "                total += B[i+1, j+1]\n",
    "            B[i, j] = total\n",
    "    return B\n",
    "\n",
    "@njit\n",
    "def avg_time_delay(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = compute_d_matrix(s, t)\n",
    "    F = compute_F_matrix(s, t, D)\n",
    "    B = compute_B_matrix(s, t, D)\n",
    "    total_delay = 0.0\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            c = cost(s[i], t[j])\n",
    "            if abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total_delay += (j - i) * F[i-1, j-1] * B[i, j]\n",
    "    num_alignments = B[0, 0]\n",
    "    if num_alignments == 0:\n",
    "        return 0.0\n",
    "    return total_delay / num_alignments\n",
    "\n",
    "def avg_time_delay_gpu(s, t):\n",
    "    s_gpu = cp.asarray(s)\n",
    "    t_gpu = cp.asarray(t)\n",
    "    T = s_gpu.shape[0]\n",
    "    D_gpu = cp.zeros((T, T), dtype=cp.float64)\n",
    "    D_gpu[0, 0] = cp.abs(s_gpu[0] - t_gpu[0])\n",
    "    for j in range(1, T):\n",
    "        D_gpu[0, j] = D_gpu[0, j-1] + cp.abs(s_gpu[0] - t_gpu[j])\n",
    "    for i in range(1, T):\n",
    "        D_gpu[i, 0] = D_gpu[i-1, 0] + cp.abs(s_gpu[i] - t_gpu[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            D_gpu[i, j] = cp.minimum(cp.minimum(D_gpu[i-1, j], D_gpu[i, j-1]),\n",
    "                                       D_gpu[i-1, j-1]) + cp.abs(s_gpu[i] - t_gpu[j])\n",
    "    D = cp.asnumpy(D_gpu)\n",
    "    return avg_time_delay(s, t)\n",
    "\n",
    "def build_propagation_graph(signal_list):\n",
    "    \"\"\"\n",
    "    Construct a propagation graph from a list of stock signals.\n",
    "    Each signal is a 1D numpy array.\n",
    "    Returns:\n",
    "        graph: dict mapping stock index to list of neighbor indices.\n",
    "        delays: dict mapping (i, j) to average delay.\n",
    "    \"\"\"\n",
    "    N = len(signal_list)\n",
    "    edges = []\n",
    "    delays = {}\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            delay_val = avg_time_delay(np.array(signal_list[i]), np.array(signal_list[j]))\n",
    "            if np.isclose(delay_val, 0.0):\n",
    "                continue\n",
    "            if delay_val > 0:\n",
    "                edges.append((i, j))\n",
    "                delays[(i, j)] = delay_val\n",
    "            else:\n",
    "                edges.append((j, i))\n",
    "                delays[(j, i)] = -delay_val\n",
    "    graph = {}\n",
    "    for (u, v) in edges:\n",
    "        graph.setdefault(u, []).append(v)\n",
    "    return graph, delays\n",
    "\n",
    "def draw_graph(graph, delays, title=\"Propagation Graph\", filename=\"graph.png\"):\n",
    "    G = nx.DiGraph()\n",
    "    for u, vs in graph.items():\n",
    "        for v in vs:\n",
    "            G.add_edge(u, v, weight=delays.get((u, v), 0))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", arrowstyle=\"->\", arrowsize=20)\n",
    "    edge_labels = {(u, v): f\"{data['weight']:.2f}\" for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def convert_prop_graph_to_edge_tensors(delays):\n",
    "    \"\"\"\n",
    "    Convert the delays dictionary into edge_index and edge_weight tensors.\n",
    "    We use exp(-delay) as weight.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    weights = []\n",
    "    for (u, v), delay in delays.items():\n",
    "        edges.append([u, v])\n",
    "        weights.append(np.exp(-delay))\n",
    "    edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "    edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "# ===================== Feature Engineering & Dataset =====================\n",
    "def compute_features(prices, window=5):\n",
    "    \"\"\"\n",
    "    Compute log return, rolling mean, and rolling std.\n",
    "    Returns a DataFrame with MultiIndex columns (stock, feature).\n",
    "    \"\"\"\n",
    "    log_ret = np.log(prices) - np.log(prices.shift(1))\n",
    "    rolling_mean = log_ret.rolling(window=window).mean()\n",
    "    rolling_std = log_ret.rolling(window=window).std()\n",
    "    \n",
    "    arrays = []\n",
    "    for stock in prices.columns:\n",
    "        for feat in ['log', 'mean', 'std']:\n",
    "            arrays.append((stock, feat))\n",
    "    col_index = pd.MultiIndex.from_tuples(arrays, names=['stock', 'feature'])\n",
    "    feature_df = pd.DataFrame(index=prices.index, columns=col_index)\n",
    "    for stock in prices.columns:\n",
    "        feature_df[(stock, 'log')] = log_ret[stock]\n",
    "        feature_df[(stock, 'mean')] = rolling_mean[stock]\n",
    "        feature_df[(stock, 'std')] = rolling_std[stock]\n",
    "    feature_df = feature_df.dropna()\n",
    "    return feature_df\n",
    "\n",
    "class StockFeatureDataset(Dataset):\n",
    "    def __init__(self, feature_df, window_size):\n",
    "        \"\"\"\n",
    "        feature_df: DataFrame with MultiIndex columns (stock, feature)\n",
    "        window_size: number of historical days.\n",
    "        For each sample, for each stock, extract a window of features (shape: (window_size, num_features)),\n",
    "        flatten it, and use as input.\n",
    "        Target: next-day log return for each stock.\n",
    "        \"\"\"\n",
    "        self.feature_df = feature_df\n",
    "        self.window_size = window_size\n",
    "        self.stocks = feature_df.columns.levels[0].tolist()\n",
    "        self.features = feature_df.columns.levels[1].tolist()\n",
    "        self.num_features = len(self.features)\n",
    "        self.num_stocks = len(self.stocks)\n",
    "        self.num_samples = feature_df.shape[0] - window_size\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, idx):\n",
    "        X_list = []\n",
    "        targets = []\n",
    "        for stock in self.stocks:\n",
    "            stock_data = self.feature_df[stock].iloc[idx: idx + self.window_size].values\n",
    "            X_list.append(stock_data.flatten())\n",
    "            targets.append(self.feature_df[stock].iloc[idx + self.window_size]['log'])\n",
    "        X = np.vstack(X_list)\n",
    "        y = np.array(targets)\n",
    "        return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# ===================== KAN Model Definition =====================\n",
    "class StockKAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "        super(StockKAN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gat = GATConv(hidden_dim, hidden_dim, heads=heads, concat=False)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.encoder(x)\n",
    "        x = self.gat(x, edge_index)\n",
    "        out = self.predictor(x)\n",
    "        return out\n",
    "\n",
    "# ===================== Backtesting Function =====================\n",
    "def backtest_model(model, test_dataset, edge_index_prop, edge_weight_prop, device, output_folder, stock_idx=0):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size, num_stocks, input_dim = X.shape\n",
    "            X_reshaped = X.view(-1, input_dim)\n",
    "            out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "            out = out.view(batch_size, num_stocks, -1)\n",
    "            predictions.append(out.squeeze().cpu().numpy())\n",
    "            actuals.append(y.cpu().numpy())\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    print(f\"Backtest RMSE: {rmse:.6f}\")\n",
    "    print(f\"Backtest R^2: {r2:.6f}\")\n",
    "\n",
    "    # Save correlation plot for specified stock\n",
    "    stock_name = test_dataset.stocks[stock_idx]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(actuals[:, stock_idx], predictions[:, stock_idx], alpha=0.6)\n",
    "    plt.xlabel(\"Actual Returns\")\n",
    "    plt.ylabel(\"Predicted Returns\")\n",
    "    plt.title(f\"Correlation Scatter for {stock_name} (Backtest)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    corr_filename = os.path.join(output_folder, f\"backtest_correlation_{stock_name}.png\")\n",
    "    plt.savefig(corr_filename)\n",
    "    plt.show()\n",
    "    print(f\"Saved backtest correlation plot to {corr_filename}\")\n",
    "    \n",
    "    # Portfolio metrics for the specified stock\n",
    "    returns = actuals[:, stock_idx]\n",
    "    mean_return = np.mean(returns)\n",
    "    std_return = np.std(returns)\n",
    "    annualized_vol = std_return * np.sqrt(252)\n",
    "    sharpe_ratio = (mean_return / std_return) * np.sqrt(252) if std_return != 0 else 0.0\n",
    "    print(f\"Backtest Metrics for {stock_name}:\")\n",
    "    print(f\"  Mean Daily Return: {mean_return:.6f}\")\n",
    "    print(f\"  Daily Return Std Dev: {std_return:.6f}\")\n",
    "    print(f\"  Annualized Volatility: {annualized_vol:.6f}\")\n",
    "    print(f\"  Sharpe Ratio: {sharpe_ratio:.6f}\")\n",
    "\n",
    "# ===================== Main Pipeline =====================\n",
    "def main():\n",
    "    # Load price data\n",
    "    df = pd.read_csv('./data/global_titans.csv')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    stocks = df.columns.tolist()\n",
    "    \n",
    "    # Compute features: log return, rolling mean, and rolling std (window=5)\n",
    "    features_df = compute_features(df, window=5)\n",
    "    \n",
    "    # Split into training and testing sets (first half for training, second half for testing)\n",
    "    T = features_df.shape[0]\n",
    "    train_features = features_df.iloc[:T//2]\n",
    "    test_features = features_df.iloc[T//2:]\n",
    "    \n",
    "    # For propagation graph, use training log returns (from features)\n",
    "    train_returns = train_features.xs('log', level='feature', axis=1)\n",
    "    train_data = train_returns.values\n",
    "    n_components = min(3, train_data.shape[1])\n",
    "    ica = FastICA(n_components=n_components, random_state=0, max_iter=5000, tol=1e-3)\n",
    "    try:\n",
    "        ica.fit(train_data)\n",
    "    except Exception as e:\n",
    "        print(\"FastICA failed on training data:\", e)\n",
    "        return\n",
    "    \n",
    "    # Build propagation graph using the provided implementation:\n",
    "    # Process data in windows (prop_window_size=30) over entire dataset\n",
    "    prop_window_size = 30\n",
    "    num_windows = df.shape[0] // prop_window_size\n",
    "    max_triplets = 10\n",
    "    all_window_avgs = []\n",
    "    \n",
    "    def process_window_prop(w):\n",
    "        window_df = df.iloc[w*prop_window_size : (w+1)*prop_window_size]\n",
    "        pair_delays = {}\n",
    "        for triplet in islice(combinations(stocks, 3), max_triplets):\n",
    "            data = window_df[list(triplet)].values  # shape: (prop_window_size, 3)\n",
    "            n_components_local = 3\n",
    "            ica_local = FastICA(n_components=n_components_local, random_state=0, max_iter=2000, tol=1e-3)\n",
    "            try:\n",
    "                S = ica_local.fit_transform(data)\n",
    "            except Exception:\n",
    "                continue\n",
    "            A = ica_local.mixing_\n",
    "            best_comp = None\n",
    "            best_spread = np.inf\n",
    "            for k in range(n_components_local):\n",
    "                spread = np.max(A[:, k]) - np.min(A[:, k])\n",
    "                if spread < best_spread:\n",
    "                    best_spread = spread\n",
    "                    best_comp = k\n",
    "            rec_signals = {}\n",
    "            for idx, stock in enumerate(triplet):\n",
    "                rec_signal = A[idx, best_comp] * S[:, best_comp]\n",
    "                rec_signals[stock] = rec_signal\n",
    "            for stock1, stock2 in combinations(triplet, 2):\n",
    "                delay_val = avg_time_delay(rec_signals[stock1], rec_signals[stock2])\n",
    "                if np.isclose(delay_val, 0.0):\n",
    "                    continue\n",
    "                if delay_val > 0:\n",
    "                    key = (stock1, stock2)\n",
    "                    val = delay_val\n",
    "                else:\n",
    "                    key = (stock2, stock1)\n",
    "                    val = -delay_val\n",
    "                pair_delays.setdefault(key, []).append(val)\n",
    "        window_avg = {pair: np.mean(vals) for pair, vals in pair_delays.items()}\n",
    "        return window_avg\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = {executor.submit(process_window_prop, w): w for w in range(num_windows)}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_window_avgs.append(result)\n",
    "            except Exception as e:\n",
    "                print(\"A window failed:\", e)\n",
    "    \n",
    "    # Aggregate delays over windows\n",
    "    aggregate_delays = {}\n",
    "    for window_avg in all_window_avgs:\n",
    "        for pair, delay_val in window_avg.items():\n",
    "            if pair in aggregate_delays:\n",
    "                s, cnt = aggregate_delays[pair]\n",
    "                aggregate_delays[pair] = (s + delay_val, cnt + 1)\n",
    "            else:\n",
    "                aggregate_delays[pair] = (delay_val, 1)\n",
    "    cumulative_avg = {pair: s/cnt for pair, (s, cnt) in aggregate_delays.items()}\n",
    "    \n",
    "    # Build propagation graph dictionary from cumulative delays\n",
    "    prop_graph = {stock: [] for stock in stocks}\n",
    "    for (u, v), avg_delay in cumulative_avg.items():\n",
    "        prop_graph.setdefault(u, []).append(v)\n",
    "    \n",
    "    # Draw and save the final propagation graph and delays CSV\n",
    "    output_folder = \"graphs\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    final_graph_file = os.path.join(output_folder, \"final_cumulative_prop_graph.png\")\n",
    "    draw_graph(prop_graph, cumulative_avg, title=\"Final Cumulative Propagation Graph\", filename=final_graph_file)\n",
    "    print(f\"Saved final propagation graph to {final_graph_file}\")\n",
    "    final_df = pd.DataFrame([(u, v, avg_delay) for (u, v), avg_delay in cumulative_avg.items()],\n",
    "                            columns=[\"Stock1\", \"Stock2\", \"AvgDelay\"])\n",
    "    final_df.to_csv(os.path.join(output_folder, \"final_cumulative_delays.csv\"), index=False)\n",
    "    print(\"Saved cumulative propagation delays CSV.\")\n",
    "    \n",
    "    # Convert propagation graph delays into edge tensors.\n",
    "    def convert_prop_graph_to_edge_tensors(delays):\n",
    "        edges = []\n",
    "        weights = []\n",
    "        for (u, v), delay in delays.items():\n",
    "            edges.append([stocks.index(u), stocks.index(v)])\n",
    "            weights.append(np.exp(-delay))\n",
    "        edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "        edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    edge_index_prop, edge_weight_prop = convert_prop_graph_to_edge_tensors(cumulative_avg)\n",
    "    print(\"Propagation graph edges (tensor):\", edge_index_prop.shape[1])\n",
    "    \n",
    "    # Create datasets using the multi-feature DataFrame\n",
    "    window_size = 60\n",
    "    train_dataset = StockFeatureDataset(train_features, window_size)\n",
    "    test_dataset = StockFeatureDataset(test_features, window_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    # Input dimension = window_size * number of features (3)\n",
    "    input_dim = window_size * 3\n",
    "    model = StockKAN(input_dim=input_dim, hidden_dim=64, output_dim=1, heads=4)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # --- Training improvements for speed:\n",
    "    # 1. Process entire batches at once.\n",
    "    # 2. Move edge tensors to the device outside the loop.\n",
    "    edge_index_prop = edge_index_prop.to(device)\n",
    "    edge_weight_prop = edge_weight_prop.to(device)\n",
    "    \n",
    "    epochs = 50\n",
    "    model.train()\n",
    "        \n",
    "    # Initialize AMP GradScaler for mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                batch_size, num_stocks, input_dim = X.shape\n",
    "                X_reshaped = X.view(-1, input_dim)\n",
    "                out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "                out = out.view(batch_size, num_stocks, -1)\n",
    "                loss = criterion(out.squeeze(), y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.6f}\")\n",
    "    \n",
    "    # Evaluate on test set (ensuring no training data is used)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size, num_stocks, input_dim = X.shape\n",
    "            X_reshaped = X.view(-1, input_dim)\n",
    "            out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "            out = out.view(batch_size, num_stocks, -1)\n",
    "            predictions.append(out.squeeze().cpu().numpy())\n",
    "            actuals.append(y.cpu().numpy())\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    print(f\"Test RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    print(f\"Test R^2: {r2:.6f}\")\n",
    "    \n",
    "    pred_sign = np.sign(predictions)\n",
    "    actual_sign = np.sign(actuals)\n",
    "    binary_accuracy = np.mean(pred_sign == actual_sign)\n",
    "    print(f\"Test Binary Accuracy: {binary_accuracy:.6f}\")\n",
    "    \n",
    "    # Print stock names\n",
    "    print(\"Stocks:\", stocks)\n",
    "    \n",
    "    # Plot correlation graph (scatter plot) for first stock\n",
    "    stock_index = 0\n",
    "    stock_name = stocks[stock_index]\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(actuals[:, stock_index], predictions[:, stock_index], alpha=0.6)\n",
    "    plt.xlabel(\"Actual Returns\")\n",
    "    plt.ylabel(\"Predicted Returns\")\n",
    "    plt.title(f\"Correlation Scatter for {stock_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    corr_filename = os.path.join(output_folder, f\"correlation_{stock_name}.png\")\n",
    "    plt.savefig(corr_filename)\n",
    "    plt.show()\n",
    "    print(f\"Saved correlation plot to {corr_filename}\")\n",
    "    \n",
    "    # Portfolio metrics on returns for first stock\n",
    "    returns = actuals[:, stock_index]\n",
    "    mean_return = np.mean(returns)\n",
    "    std_return = np.std(returns)\n",
    "    annualized_vol = std_return * np.sqrt(252)\n",
    "    sharpe_ratio = (mean_return / std_return) * np.sqrt(252) if std_return != 0 else 0.0\n",
    "    print(f\"Metrics for {stock_name}:\")\n",
    "    print(f\"  Mean Daily Return: {mean_return:.6f}\")\n",
    "    print(f\"  Daily Return Std Dev: {std_return:.6f}\")\n",
    "    print(f\"  Annualized Volatility: {annualized_vol:.6f}\")\n",
    "    print(f\"  Sharpe Ratio: {sharpe_ratio:.6f}\")\n",
    "    \n",
    "    # Plot predicted vs actual returns for first stock\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(predictions[:, stock_index], label=\"Predicted\")\n",
    "    plt.plot(actuals[:, stock_index], label=\"Actual\")\n",
    "    plt.xlabel(\"Test Sample Index\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    plt.title(f\"Predicted vs Actual Returns for {stock_name} (2 Years)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot cumulative PnL for first stock\n",
    "    positions = np.sign(predictions[:, stock_index])\n",
    "    daily_pnl = positions * actuals[:, stock_index]\n",
    "    cumulative_pnl = np.cumsum(daily_pnl)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(cumulative_pnl, marker='o', linestyle='-', label=\"Cumulative PnL\")\n",
    "    plt.xlabel(\"Test Sample Index (Days)\")\n",
    "    plt.ylabel(\"Cumulative PnL\")\n",
    "    plt.title(f\"PnL Chart for {stock_name} (2 Years)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the model for future backtesting\n",
    "    model_path = os.path.join(\"saved_models\", \"stockkan_model.pth\")\n",
    "    os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved model to {model_path}\")\n",
    "    \n",
    "    # Backtesting on test data (different time ranges can be used by creating different test datasets)\n",
    "    print(\"Starting backtesting on test data...\")\n",
    "    backtest_model(model, test_dataset, edge_index_prop, edge_weight_prop, device, output_folder, stock_idx=0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40ee8250-6b14-44f7-a17d-d4f73d51ef6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICA failed on training data: eigh() got an unexpected keyword argument 'eigvals'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import combinations, islice\n",
    "from sklearn.decomposition import FastICA\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from numba import njit\n",
    "import cupy as cp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Workaround for torch_geometric compatibility ---\n",
    "if not hasattr(torch.serialization, 'add_safe_globals'):\n",
    "    torch.serialization.add_safe_globals = lambda x: None\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ===================== Propagation Graph Implementation =====================\n",
    "@njit\n",
    "def cost(a, b):\n",
    "    return abs(a - b)\n",
    "\n",
    "@njit\n",
    "def compute_d_matrix(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = np.empty((T, T), dtype=np.float64)\n",
    "    D[0, 0] = cost(s[0], t[0])\n",
    "    for j in range(1, T):\n",
    "        D[0, j] = D[0, j-1] + cost(s[0], t[j])\n",
    "    for i in range(1, T):\n",
    "        D[i, 0] = D[i-1, 0] + cost(s[i], t[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            a_val = D[i-1, j]\n",
    "            b_val = D[i, j-1]\n",
    "            c_val = D[i-1, j-1]\n",
    "            D[i, j] = min(a_val, b_val, c_val) + cost(s[i], t[j])\n",
    "    return D\n",
    "\n",
    "@njit\n",
    "def compute_F_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    F = np.zeros((T, T), dtype=np.float64)\n",
    "    F[0, 0] = 1.0\n",
    "    for i in range(T):\n",
    "        for j in range(T):\n",
    "            if i == 0 and j == 0:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            c = cost(s[i], t[j])\n",
    "            if i > 0 and abs(D[i, j] - (D[i-1, j] + c)) < 1e-8:\n",
    "                total += F[i-1, j]\n",
    "            if j > 0 and abs(D[i, j] - (D[i, j-1] + c)) < 1e-8:\n",
    "                total += F[i, j-1]\n",
    "            if i > 0 and j > 0 and abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total += F[i-1, j-1]\n",
    "            F[i, j] = total\n",
    "    return F\n",
    "\n",
    "@njit\n",
    "def compute_B_matrix(s, t, D):\n",
    "    T = s.shape[0]\n",
    "    B = np.zeros((T, T), dtype=np.float64)\n",
    "    B[T-1, T-1] = 1.0\n",
    "    for i in range(T-1, -1, -1):\n",
    "        for j in range(T-1, -1, -1):\n",
    "            if i == T-1 and j == T-1:\n",
    "                continue\n",
    "            total = 0.0\n",
    "            if i+1 < T and abs(D[i+1, j] - (D[i, j] + cost(s[i+1], t[j]))) < 1e-8:\n",
    "                total += B[i+1, j]\n",
    "            if j+1 < T and abs(D[i, j+1] - (D[i, j] + cost(s[i], t[j+1]))) < 1e-8:\n",
    "                total += B[i, j+1]\n",
    "            if i+1 < T and j+1 < T and abs(D[i+1, j+1] - (D[i, j] + cost(s[i+1], t[j+1]))) < 1e-8:\n",
    "                total += B[i+1, j+1]\n",
    "            B[i, j] = total\n",
    "    return B\n",
    "\n",
    "@njit\n",
    "def avg_time_delay(s, t):\n",
    "    T = s.shape[0]\n",
    "    D = compute_d_matrix(s, t)\n",
    "    F = compute_F_matrix(s, t, D)\n",
    "    B = compute_B_matrix(s, t, D)\n",
    "    total_delay = 0.0\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            c = cost(s[i], t[j])\n",
    "            if abs(D[i, j] - (D[i-1, j-1] + c)) < 1e-8:\n",
    "                total_delay += (j - i) * F[i-1, j-1] * B[i, j]\n",
    "    num_alignments = B[0, 0]\n",
    "    if num_alignments == 0:\n",
    "        return 0.0\n",
    "    return total_delay / num_alignments\n",
    "\n",
    "def avg_time_delay_gpu(s, t):\n",
    "    s_gpu = cp.asarray(s)\n",
    "    t_gpu = cp.asarray(t)\n",
    "    T = s_gpu.shape[0]\n",
    "    D_gpu = cp.zeros((T, T), dtype=cp.float64)\n",
    "    D_gpu[0, 0] = cp.abs(s_gpu[0] - t_gpu[0])\n",
    "    for j in range(1, T):\n",
    "        D_gpu[0, j] = D_gpu[0, j-1] + cp.abs(s_gpu[0] - t_gpu[j])\n",
    "    for i in range(1, T):\n",
    "        D_gpu[i, 0] = D_gpu[i-1, 0] + cp.abs(s_gpu[i] - t_gpu[0])\n",
    "    for i in range(1, T):\n",
    "        for j in range(1, T):\n",
    "            D_gpu[i, j] = cp.minimum(cp.minimum(D_gpu[i-1, j], D_gpu[i, j-1]),\n",
    "                                       D_gpu[i-1, j-1]) + cp.abs(s_gpu[i] - t_gpu[j])\n",
    "    D = cp.asnumpy(D_gpu)\n",
    "    return avg_time_delay(s, t)\n",
    "\n",
    "def build_propagation_graph(signal_list):\n",
    "    \"\"\"\n",
    "    Construct a propagation graph from a list of stock signals.\n",
    "    Each signal is a 1D numpy array.\n",
    "    Returns:\n",
    "        graph: dict mapping stock index to list of neighbor indices.\n",
    "        delays: dict mapping (i, j) to average delay.\n",
    "    \"\"\"\n",
    "    N = len(signal_list)\n",
    "    edges = []\n",
    "    delays = {}\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            delay_val = avg_time_delay(np.array(signal_list[i]), np.array(signal_list[j]))\n",
    "            if np.isclose(delay_val, 0.0):\n",
    "                continue\n",
    "            if delay_val > 0:\n",
    "                edges.append((i, j))\n",
    "                delays[(i, j)] = delay_val\n",
    "            else:\n",
    "                edges.append((j, i))\n",
    "                delays[(j, i)] = -delay_val\n",
    "    graph = {}\n",
    "    for (u, v) in edges:\n",
    "        graph.setdefault(u, []).append(v)\n",
    "    return graph, delays\n",
    "\n",
    "def draw_graph(graph, delays, title=\"Propagation Graph\", filename=\"graph.png\"):\n",
    "    G = nx.DiGraph()\n",
    "    for u, vs in graph.items():\n",
    "        for v in vs:\n",
    "            G.add_edge(u, v, weight=delays.get((u, v), 0))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", arrowstyle=\"->\", arrowsize=20)\n",
    "    edge_labels = {(u, v): f\"{data['weight']:.2f}\" for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def convert_prop_graph_to_edge_tensors(delays):\n",
    "    \"\"\"\n",
    "    Convert the delays dictionary into edge_index and edge_weight tensors.\n",
    "    We use exp(-delay) as weight.\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    weights = []\n",
    "    for (u, v), delay in delays.items():\n",
    "        edges.append([u, v])\n",
    "        weights.append(np.exp(-delay))\n",
    "    edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "    edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "# ===================== Feature Engineering & Dataset =====================\n",
    "def compute_features(prices, window=5):\n",
    "    \"\"\"\n",
    "    Compute log return, rolling mean, and rolling std.\n",
    "    Returns a DataFrame with MultiIndex columns (stock, feature).\n",
    "    \"\"\"\n",
    "    log_ret = np.log(prices) - np.log(prices.shift(1))\n",
    "    rolling_mean = log_ret.rolling(window=window).mean()\n",
    "    rolling_std = log_ret.rolling(window=window).std()\n",
    "    \n",
    "    arrays = []\n",
    "    for stock in prices.columns:\n",
    "        for feat in ['log', 'mean', 'std']:\n",
    "            arrays.append((stock, feat))\n",
    "    col_index = pd.MultiIndex.from_tuples(arrays, names=['stock', 'feature'])\n",
    "    feature_df = pd.DataFrame(index=prices.index, columns=col_index)\n",
    "    for stock in prices.columns:\n",
    "        feature_df[(stock, 'log')] = log_ret[stock]\n",
    "        feature_df[(stock, 'mean')] = rolling_mean[stock]\n",
    "        feature_df[(stock, 'std')] = rolling_std[stock]\n",
    "    feature_df = feature_df.dropna()\n",
    "    return feature_df\n",
    "\n",
    "class StockFeatureDataset(Dataset):\n",
    "    def __init__(self, feature_df, window_size):\n",
    "        \"\"\"\n",
    "        feature_df: DataFrame with MultiIndex columns (stock, feature)\n",
    "        window_size: number of historical days.\n",
    "        For each sample, for each stock, extract a window of features (shape: (window_size, num_features)),\n",
    "        flatten it, and use as input.\n",
    "        Target: next-day log return for each stock.\n",
    "        \"\"\"\n",
    "        self.feature_df = feature_df\n",
    "        self.window_size = window_size\n",
    "        self.stocks = feature_df.columns.levels[0].tolist()\n",
    "        self.features = feature_df.columns.levels[1].tolist()\n",
    "        self.num_features = len(self.features)\n",
    "        self.num_stocks = len(self.stocks)\n",
    "        self.num_samples = feature_df.shape[0] - window_size\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, idx):\n",
    "        X_list = []\n",
    "        targets = []\n",
    "        for stock in self.stocks:\n",
    "            stock_data = self.feature_df[stock].iloc[idx: idx + self.window_size].values\n",
    "            X_list.append(stock_data.flatten())\n",
    "            targets.append(self.feature_df[stock].iloc[idx + self.window_size]['log'])\n",
    "        X = np.vstack(X_list)\n",
    "        y = np.array(targets)\n",
    "        return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "# ===================== KAN Model Definition =====================\n",
    "class StockKAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
    "        super(StockKAN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.gat = GATConv(hidden_dim, hidden_dim, heads=heads, concat=False)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.encoder(x)\n",
    "        x = self.gat(x, edge_index)\n",
    "        out = self.predictor(x)\n",
    "        return out\n",
    "\n",
    "# ===================== Backtesting Function for All Stocks =====================\n",
    "def backtest_all_stocks(model, test_dataset, edge_index_prop, edge_weight_prop, device, output_folder):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size, num_stocks, input_dim = X.shape\n",
    "            X_reshaped = X.view(-1, input_dim)\n",
    "            out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "            out = out.view(batch_size, num_stocks, -1)\n",
    "            predictions.append(out.squeeze().cpu().numpy())\n",
    "            actuals.append(y.cpu().numpy())\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    \n",
    "    portfolio_metrics = \"\"\n",
    "    # Generate and save PnL and portfolio statistics for each stock\n",
    "    for stock_idx, stock_name in enumerate(test_dataset.stocks):\n",
    "        stock_preds = predictions[:, stock_idx]\n",
    "        stock_actuals = actuals[:, stock_idx]\n",
    "        rmse = np.sqrt(np.mean((stock_preds - stock_actuals)**2))\n",
    "        r2 = r2_score(stock_actuals, stock_preds)\n",
    "        metrics_str = f\"Stock: {stock_name}\\n  RMSE: {rmse:.6f}\\n  R^2: {r2:.6f}\\n\"\n",
    "        print(f\"Backtest metrics for {stock_name}:\")\n",
    "        print(metrics_str)\n",
    "        \n",
    "        # Correlation scatter plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(stock_actuals, stock_preds, alpha=0.6)\n",
    "        plt.xlabel(\"Actual Returns\")\n",
    "        plt.ylabel(\"Predicted Returns\")\n",
    "        plt.title(f\"Correlation Scatter for {stock_name} (Backtest)\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        corr_filename = os.path.join(output_folder, f\"backtest_correlation_{stock_name}.png\")\n",
    "        plt.savefig(corr_filename)\n",
    "        plt.close()\n",
    "        print(f\"Saved backtest correlation plot to {corr_filename}\")\n",
    "        \n",
    "        # Portfolio metrics\n",
    "        mean_return = np.mean(stock_actuals)\n",
    "        std_return = np.std(stock_actuals)\n",
    "        annualized_vol = std_return * np.sqrt(252)\n",
    "        sharpe_ratio = (mean_return / std_return) * np.sqrt(252) if std_return != 0 else 0.0\n",
    "        metrics_str += (f\"  Mean Daily Return: {mean_return:.6f}\\n\" +\n",
    "                        f\"  Daily Return Std Dev: {std_return:.6f}\\n\" +\n",
    "                        f\"  Annualized Volatility: {annualized_vol:.6f}\\n\" +\n",
    "                        f\"  Sharpe Ratio: {sharpe_ratio:.6f}\\n\\n\")\n",
    "        print(f\"Backtest Portfolio Metrics for {stock_name}:\\n\"\n",
    "              f\"  Mean Daily Return: {mean_return:.6f}\\n\"\n",
    "              f\"  Daily Return Std Dev: {std_return:.6f}\\n\"\n",
    "              f\"  Annualized Volatility: {annualized_vol:.6f}\\n\"\n",
    "              f\"  Sharpe Ratio: {sharpe_ratio:.6f}\")\n",
    "        \n",
    "        # Plot cumulative PnL\n",
    "        positions = np.sign(stock_preds)\n",
    "        daily_pnl = positions * stock_actuals\n",
    "        cumulative_pnl = np.cumsum(daily_pnl)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(cumulative_pnl, marker='o', linestyle='-', label=\"Cumulative PnL\")\n",
    "        plt.xlabel(\"Test Sample Index (Days)\")\n",
    "        plt.ylabel(\"Cumulative PnL\")\n",
    "        plt.title(f\"PnL Chart for {stock_name} (Backtest)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        pnl_filename = os.path.join(output_folder, f\"backtest_pnl_{stock_name}.png\")\n",
    "        plt.savefig(pnl_filename)\n",
    "        plt.close()\n",
    "        print(f\"Saved backtest PnL chart to {pnl_filename}\")\n",
    "        \n",
    "        portfolio_metrics += metrics_str\n",
    "\n",
    "    # Save portfolio metrics to a text file in the graphs folder\n",
    "    metrics_file = os.path.join(output_folder, \"portfolio_metrics.txt\")\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        f.write(portfolio_metrics)\n",
    "    print(f\"Saved portfolio metrics to {metrics_file}\")\n",
    "\n",
    "# ===================== Main Pipeline =====================\n",
    "def main():\n",
    "    # Load price data\n",
    "    df = pd.read_csv('./data/global_titans.csv')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    stocks = df.columns.tolist()\n",
    "    \n",
    "    # Compute features: log return, rolling mean, and rolling std (window=5)\n",
    "    features_df = compute_features(df, window=5)\n",
    "    \n",
    "    # Split into training and testing sets (first half for training, second half for testing)\n",
    "    T = features_df.shape[0]\n",
    "    train_features = features_df.iloc[:T//2]\n",
    "    test_features = features_df.iloc[T//2:]\n",
    "    \n",
    "    # For propagation graph, use training log returns (from features)\n",
    "    train_returns = train_features.xs('log', level='feature', axis=1)\n",
    "    train_data = train_returns.values\n",
    "    n_components = min(3, train_data.shape[1])\n",
    "    ica = FastICA(n_components=n_components, random_state=0, max_iter=5000, tol=1e-3)\n",
    "    try:\n",
    "        ica.fit(train_data)\n",
    "    except Exception as e:\n",
    "        print(\"FastICA failed on training data:\", e)\n",
    "        return\n",
    "    \n",
    "    # Build propagation graph using the provided implementation:\n",
    "    # Process data in windows (prop_window_size=30) over entire dataset\n",
    "    prop_window_size = 30\n",
    "    num_windows = df.shape[0] // prop_window_size\n",
    "    max_triplets = 10\n",
    "    all_window_avgs = []\n",
    "    \n",
    "    def process_window_prop(w):\n",
    "        window_df = df.iloc[w*prop_window_size : (w+1)*prop_window_size]\n",
    "        pair_delays = {}\n",
    "        for triplet in islice(combinations(stocks, 3), max_triplets):\n",
    "            data = window_df[list(triplet)].values  # shape: (prop_window_size, 3)\n",
    "            n_components_local = 3\n",
    "            ica_local = FastICA(n_components=n_components_local, random_state=0, max_iter=5000, tol=1e-3)\n",
    "            try:\n",
    "                S = ica_local.fit_transform(data)\n",
    "            except Exception:\n",
    "                continue\n",
    "            A = ica_local.mixing_\n",
    "            best_comp = None\n",
    "            best_spread = np.inf\n",
    "            for k in range(n_components_local):\n",
    "                spread = np.max(A[:, k]) - np.min(A[:, k])\n",
    "                if spread < best_spread:\n",
    "                    best_spread = spread\n",
    "                    best_comp = k\n",
    "            rec_signals = {}\n",
    "            for idx, stock in enumerate(triplet):\n",
    "                rec_signal = A[idx, best_comp] * S[:, best_comp]\n",
    "                rec_signals[stock] = rec_signal\n",
    "            for stock1, stock2 in combinations(triplet, 2):\n",
    "                delay_val = avg_time_delay(rec_signals[stock1], rec_signals[stock2])\n",
    "                if np.isclose(delay_val, 0.0):\n",
    "                    continue\n",
    "                if delay_val > 0:\n",
    "                    key = (stock1, stock2)\n",
    "                    val = delay_val\n",
    "                else:\n",
    "                    key = (stock2, stock1)\n",
    "                    val = -delay_val\n",
    "                pair_delays.setdefault(key, []).append(val)\n",
    "        window_avg = {pair: np.mean(vals) for pair, vals in pair_delays.items()}\n",
    "        return window_avg\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = {executor.submit(process_window_prop, w): w for w in range(num_windows)}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_window_avgs.append(result)\n",
    "            except Exception as e:\n",
    "                print(\"A window failed:\", e)\n",
    "    \n",
    "    # Aggregate delays over windows\n",
    "    aggregate_delays = {}\n",
    "    for window_avg in all_window_avgs:\n",
    "        for pair, delay_val in window_avg.items():\n",
    "            if pair in aggregate_delays:\n",
    "                s, cnt = aggregate_delays[pair]\n",
    "                aggregate_delays[pair] = (s + delay_val, cnt + 1)\n",
    "            else:\n",
    "                aggregate_delays[pair] = (delay_val, 1)\n",
    "    cumulative_avg = {pair: s/cnt for pair, (s, cnt) in aggregate_delays.items()}\n",
    "    \n",
    "    # Build propagation graph dictionary from cumulative delays\n",
    "    prop_graph = {stock: [] for stock in stocks}\n",
    "    for (u, v), avg_delay in cumulative_avg.items():\n",
    "        prop_graph.setdefault(u, []).append(v)\n",
    "    \n",
    "    # Draw and save the final propagation graph and delays CSV\n",
    "    output_folder = \"graphs\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    final_graph_file = os.path.join(output_folder, \"final_cumulative_prop_graph.png\")\n",
    "    draw_graph(prop_graph, cumulative_avg, title=\"Final Cumulative Propagation Graph\", filename=final_graph_file)\n",
    "    print(f\"Saved final propagation graph to {final_graph_file}\")\n",
    "    final_df = pd.DataFrame([(u, v, avg_delay) for (u, v), avg_delay in cumulative_avg.items()],\n",
    "                            columns=[\"Stock1\", \"Stock2\", \"AvgDelay\"])\n",
    "    final_df.to_csv(os.path.join(output_folder, \"final_cumulative_delays.csv\"), index=False)\n",
    "    print(\"Saved cumulative propagation delays CSV.\")\n",
    "    \n",
    "    # Convert propagation graph delays into edge tensors.\n",
    "    def convert_prop_graph_to_edge_tensors(delays):\n",
    "        edges = []\n",
    "        weights = []\n",
    "        for (u, v), delay in delays.items():\n",
    "            edges.append([stocks.index(u), stocks.index(v)])\n",
    "            weights.append(np.exp(-delay))\n",
    "        edge_index = torch.tensor(np.array(edges).T, dtype=torch.long)\n",
    "        edge_weight = torch.tensor(weights, dtype=torch.float)\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    edge_index_prop, edge_weight_prop = convert_prop_graph_to_edge_tensors(cumulative_avg)\n",
    "    print(\"Propagation graph edges (tensor):\", edge_index_prop.shape[1])\n",
    "    \n",
    "    # Create datasets using the multi-feature DataFrame\n",
    "    window_size = 60\n",
    "    train_dataset = StockFeatureDataset(train_features, window_size)\n",
    "    test_dataset = StockFeatureDataset(test_features, window_size)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    # Input dimension = window_size * number of features (3)\n",
    "    input_dim = window_size * 3\n",
    "    model = StockKAN(input_dim=input_dim, hidden_dim=64, output_dim=1, heads=4)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # --- Training improvements for speed:\n",
    "    # 1. Process entire batches at once.\n",
    "    # 2. Move edge tensors to the device outside the loop.\n",
    "    edge_index_prop = edge_index_prop.to(device)\n",
    "    edge_weight_prop = edge_weight_prop.to(device)\n",
    "    \n",
    "    epochs = 50\n",
    "    model.train()\n",
    "        \n",
    "    # Initialize AMP GradScaler for mixed precision training\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                batch_size, num_stocks, input_dim = X.shape\n",
    "                X_reshaped = X.view(-1, input_dim)\n",
    "                out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "                out = out.view(batch_size, num_stocks, -1)\n",
    "                loss = criterion(out.squeeze(), y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.6f}\")\n",
    "    \n",
    "    # Evaluate on test set (ensuring no training data is used)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size, num_stocks, input_dim = X.shape\n",
    "            X_reshaped = X.view(-1, input_dim)\n",
    "            out = model(X_reshaped, edge_index_prop, edge_weight_prop)\n",
    "            out = out.view(batch_size, num_stocks, -1)\n",
    "            predictions.append(out.squeeze().cpu().numpy())\n",
    "            actuals.append(y.cpu().numpy())\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    print(f\"Test RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    print(f\"Test R^2: {r2:.6f}\")\n",
    "    \n",
    "    pred_sign = np.sign(predictions)\n",
    "    actual_sign = np.sign(actuals)\n",
    "    binary_accuracy = np.mean(pred_sign == actual_sign)\n",
    "    print(f\"Test Binary Accuracy: {binary_accuracy:.6f}\")\n",
    "    \n",
    "    # Print stock names\n",
    "    print(\"Stocks:\", stocks)\n",
    "    \n",
    "    # Save the model for future backtesting\n",
    "    model_path = os.path.join(\"saved_models\", \"stockkan_model.pth\")\n",
    "    os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved model to {model_path}\")\n",
    "    \n",
    "    # Backtest and generate PnL and statistics for each stock in the dataset,\n",
    "    # and save portfolio metrics in a text file.\n",
    "    print(\"Starting backtesting on test data for all stocks...\")\n",
    "    backtest_all_stocks(model, test_dataset, edge_index_prop, edge_weight_prop, device, output_folder)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f727f-bbf5-4eaf-b69b-c12cce2d830d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
